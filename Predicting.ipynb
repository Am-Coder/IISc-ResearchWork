{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from window_slider import Slider\n",
    "from scipy import signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437, 30)\n",
      "<bound method NDFrame.head of 54     1\n",
      "376    5\n",
      "229    2\n",
      "2      1\n",
      "91     1\n",
      "216    2\n",
      "113    1\n",
      "315    3\n",
      "239    2\n",
      "70     1\n",
      "6      1\n",
      "281    3\n",
      "23     1\n",
      "0      1\n",
      "346    4\n",
      "224    2\n",
      "114    1\n",
      "61     1\n",
      "291    3\n",
      "172    1\n",
      "37     1\n",
      "194    2\n",
      "108    1\n",
      "184    2\n",
      "282    3\n",
      "187    2\n",
      "236    2\n",
      "271    3\n",
      "136    1\n",
      "250    2\n",
      "      ..\n",
      "203    2\n",
      "400    5\n",
      "305    3\n",
      "377    5\n",
      "225    2\n",
      "103    1\n",
      "45     1\n",
      "251    2\n",
      "133    1\n",
      "317    3\n",
      "209    2\n",
      "235    2\n",
      "352    4\n",
      "182    2\n",
      "348    4\n",
      "47     1\n",
      "333    4\n",
      "160    1\n",
      "62     1\n",
      "214    2\n",
      "193    2\n",
      "207    2\n",
      "205    2\n",
      "270    3\n",
      "433    5\n",
      "177    1\n",
      "332    4\n",
      "30     1\n",
      "179    1\n",
      "367    5\n",
      "Name: 30, Length: 437, dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"E:/games/IISc-May-2019/HAPT Data Set/FeatureSet/HypIMU_time_0overlap_feature_Dataset.csv\",header=None)\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "\n",
    "\n",
    "# y = df.loc[:,108]    #for 30 features\n",
    "# X = df.loc[:,0:107]\n",
    "\n",
    "# y = df.loc[:,200]    #for 200 features\n",
    "# X = df.loc[:,0:199]  \n",
    "\n",
    "\n",
    "y = df.loc[:,30]    #for 30 features\n",
    "X = df.loc[:,0:29]\n",
    "\n",
    "# y = df.loc[:,100]    #for 100 features\n",
    "# X = df.loc[:,0:99]\n",
    "\n",
    "\n",
    "# y = df.loc[:,60]      #for 60 or 60-0 features\n",
    "# X = df.loc[:,0:59]\n",
    "\n",
    "# y = df.loc[:, 36]        #for 36-0 features\n",
    "# X = df.loc[:, 0:35]\n",
    "\n",
    "# y = df.loc[:,120]      #for 60-60 features\n",
    "# X = df.loc[:,0:119]\n",
    "\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "X = standard_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uci_X = pd.read_csv(\"E:/games/IISc-May-2019/HAPT Data Set/Train/X_train.csv\")\n",
    "uci_y = pd.read_csv(\"E:/games/IISc-May-2019/HAPT Data Set/Train/y_train.csv\")\n",
    "\n",
    "# uci_X = uci_X.sample(frac=1).reset_index(drop=True)\n",
    "# uci_y = uci_y.sample(frac=1).reset_index(drop=True)\n",
    "uci_X = uci_X.loc[0:5000,:]\n",
    "uci_y = uci_y.loc[0:5000,:]\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.head)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9337147415997382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.91111111, 0.95505618, 0.93181818, 0.90588235, 0.96470588])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# clf = SVC(kernel='linear', C=100 )\n",
    "clf = SVC( kernel = 'rbf', gamma=.01, C = 10, decision_function_shape = 'ovr' )\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44,) (393,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, shuffle=True) \n",
    "print(y_test.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5128617363344051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 81,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 51,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  3,   0,   0,   0,  81,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,   0,   0,   0, 132,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 105,   0,   0,   0,   0,   0,   0],\n",
       "       [ 12,   0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 19,   0,   0,   0,   1,   2,   0,   0,   0,   0,   0,   0],\n",
       "       [ 14,   0,   0,   0,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 26,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0],\n",
       "       [ 12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train) \n",
    "dtree_predictions = dtree_model.predict(X_test) \n",
    "accuracy = dtree_model.score(X_test, y_test) \n",
    "\n",
    "# creating a confusion matrix \n",
    "cm = confusion_matrix(y_test, dtree_predictions)\n",
    "\n",
    "print(accuracy)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949109414758269\n",
      "0.9318181818181818\n",
      "[[22  0  2  0  0]\n",
      " [ 0  3  1  0  0]\n",
      " [ 0  0  4  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  0  0  7]]\n",
      "0.9720101781170484\n",
      "0.9772727272727273\n",
      "[[24  0  0  0  0]\n",
      " [ 0  3  1  0  0]\n",
      " [ 0  0  4  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "svm_model_linear = SVC(kernel = 'linear', C = 100 ).fit(X_train, y_train)  #lesser the c hyperparameter , softer the margin\n",
    "                                                                            #higher c means hard margin\n",
    "\n",
    "\n",
    "svm_model_rbf = SVC(kernel = 'rbf', gamma=.01, C = 10, decision_function_shape = 'ovr').fit(X_train, y_train)#gamma makes bell shaped curve narrower and as \n",
    "# a result influence of each term becomes smaller. Increasin gamma may lead to overfitting\n",
    "\n",
    "#The RBF kernel SVM decision region is actually also a linear decision region. What RBF kernel SVM actually does is to create \n",
    "# non-linear combinations of your features to uplift your samples onto a higher-dimensional feature space where you can use a \n",
    "# linear decision boundary to separate your classes and then invert this boundary back to lower dimension.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svm_model_linear = SVC(kernel = 'poly', degree=3, coef0=100, C = 5).fit(X_train, y_train)#coef0 controls how much our model is \n",
    "                                                                            #influenced by high degree poly vs low degree poly\n",
    "svm_predictions1 = svm_model_linear.predict(X_test) \n",
    "svm_predictions2 = svm_model_rbf.predict(X_test) \n",
    "\n",
    "# model accuracy for X_test   \n",
    "accuracy1 = svm_model_linear.score(X_test, y_test) \n",
    "accuracy2 = svm_model_rbf.score(X_test, y_test) \n",
    "\n",
    "# training accuracy\n",
    "taccu1 = svm_model_linear.score(X_train, y_train)\n",
    "taccu2 = svm_model_rbf.score(X_train, y_train)\n",
    "\n",
    "# creating a confusion matrix \n",
    "cm1 = confusion_matrix(y_test, svm_predictions1)\n",
    "cm2 = confusion_matrix(y_test, svm_predictions2)\n",
    "\n",
    "print(taccu1)\n",
    "print(accuracy1)\n",
    "print(cm1)\n",
    "\n",
    "print(taccu2)\n",
    "print(accuracy2)\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9646182495344506\n",
      "0.9599627560521415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[433,   1,   6,   0,   0],\n",
       "       [ 11, 175,   3,   0,   0],\n",
       "       [ 19,   1, 133,   0,   2],\n",
       "       [  0,   0,   0, 108,   0],\n",
       "       [  0,   0,   0,   0, 182]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training a KNN classifier \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit( X_train, y_train ) \n",
    "\n",
    "cm = confusion_matrix(y_test, knn.predict(X_test))\n",
    "\n",
    "# training accuracy\n",
    "taccu = knn.score(X_train, y_train)\n",
    "\n",
    "# accuracy on X_test\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "\n",
    "print(taccu)\n",
    "print(accuracy)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9723577235772358\n",
      "0.9415584415584416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0, 43,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, 46,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 55,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0, 15,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  8]], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, multi_class='ovr')\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "cm = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "# training accuracy\n",
    "taccu = clf.score(X_train, y_train)\n",
    "\n",
    "# accuracy on X_test\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(taccu)\n",
    "print(accuracy)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9235772357723577\n",
      "0.8798701298701299\n"
     ]
    }
   ],
   "source": [
    "# training a Naive Bayes classifier \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB().fit(X_train, y_train) \n",
    "gnb_predictions = gnb.predict(X_test) \n",
    "\n",
    "# training accuracy\n",
    "taccu = gnb.score(X_train, y_train)\n",
    "\n",
    "# accuracy on X_test \n",
    "accuracy = gnb.score(X_test, y_test) \n",
    "\n",
    "print(taccu)\n",
    "print(accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9813008130081301\n",
      "0.948051948051948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 45,  8,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, 46,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 55,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0, 15,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  7]], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "clf_predictions = clf.predict(X_test)\n",
    "\n",
    "# training accuracy\n",
    "taccu = clf.score(X_train, y_train)\n",
    "\n",
    "\n",
    "# accuracy on X_test \n",
    "cm = confusion_matrix(y_test, clf_predictions)\n",
    "accu = clf.score( X_test, y_test )\n",
    "\n",
    "\n",
    "print(taccu)\n",
    "print(accu)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9699186991869919\n",
      "0.9383116883116883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 45,  8,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, 46,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 55,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0, 15,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  7]], dtype=int64)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(max_iter=4000, tol=1e-3)\n",
    "clf.fit( X_train, y_train )\n",
    "\n",
    "# training accuracy\n",
    "taccu = clf.score(X_train, y_train)\n",
    "\n",
    "\n",
    "# accuracy on X_test \n",
    "cm = confusion_matrix(y_test, clf_predictions)\n",
    "accu = clf.score( X_test, y_test )\n",
    "\n",
    "\n",
    "print(taccu)\n",
    "print(accu)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967479674796748\n",
      "0.9285714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 45,  8,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, 46,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 55,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0, 15,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  7]], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "clf = OneVsRestClassifier(linear_model.SGDClassifier(max_iter=4000, tol=1e-3))\n",
    "clf.fit( X_train, y_train )\n",
    "\n",
    "# training accuracy\n",
    "taccu = clf.score(X_train, y_train)\n",
    "\n",
    "\n",
    "# accuracy on X_test \n",
    "cm = confusion_matrix(y_test, clf_predictions)\n",
    "accu = clf.score( X_test, y_test )\n",
    "\n",
    "\n",
    "print(taccu)\n",
    "print(accu)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1116, 15)\n",
      "(1116, 15)\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[3]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[2]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[3]\n",
      "[2]\n",
      "[1]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# This is for testing on test set .\n",
    "\n",
    "\n",
    "# df1\n",
    "# for i in range(1,13):\n",
    "# accfile = source + str(i) + acc + '.csv'\n",
    "#     gyrofile = source + str(i) + gyro + '.csv'\n",
    "#     df1 = pd.read_csv(accfile)\n",
    "#     df2 = pd.read_csv(gyrofile)\n",
    "df1 = pd.read_csv(\"E:/games/IISc-May-2019/HAPT Data Set/MobileTest/Test1.csv\")\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "scaled_df = standard_scaler.fit_transform(df1)\n",
    "df1 = pd.DataFrame(scaled_df, columns=[ 'ax', 'ay', 'az', 'wx', 'wy', 'wz' ])\n",
    "\n",
    "ax = df1['ax']\n",
    "ay = df1['ay']\n",
    "az = df1['az']\n",
    "wx = df1['wx']\n",
    "wy = df1['wy']\n",
    "wz = df1['wz']\n",
    "fs = 50\n",
    "#------------------------------------------------------------------------------------------------------------------- \n",
    "#Time domain\n",
    "timeAcc = np.empty([0,15])\n",
    "timeGyro = np.empty([0,15])\n",
    "\n",
    "size = 25\n",
    "overlap = 20\n",
    "\n",
    "\n",
    "for k in range(2):\n",
    "    slider1 = Slider(size, overlap)\n",
    "    slider2 = Slider(size, overlap)\n",
    "    slider3 = Slider(size, overlap)\n",
    "    if(k == 0):\n",
    "        slider1.fit(ax)\n",
    "        slider2.fit(ay)\n",
    "        slider3.fit(az)\n",
    "    elif(k == 1):\n",
    "        slider1.fit(wx)\n",
    "        slider2.fit(wy)\n",
    "        slider3.fit(wz)        \n",
    "    while True:\n",
    "        window_data1 = slider1.slide()\n",
    "        window_data2 = slider2.slide()\n",
    "        window_data3 = slider3.slide()\n",
    "\n",
    "        # do your stuff\n",
    "        mean1 = np.mean(window_data1)\n",
    "        variance1 = np.var(window_data1)\n",
    "        median1 = np.median(window_data1)\n",
    "        rms1 = np.sqrt(np.mean(window_data1**2))\n",
    "#             skewdness1 = skew(window_data1)\n",
    "#             kurtos1 = kurtosis(window_data1)\n",
    "        ptp1 = np.ptp(window_data1)\n",
    "#         rang1 = np.max(window_data1) - np.min(window_data1)\n",
    "#             interq_range1 = iqr(window_data1)\n",
    "#             crest_factor1 = np.max(window_data1)/rms1\n",
    "\n",
    "        mean2 = np.mean(window_data2)\n",
    "        variance2 = np.var(window_data2)\n",
    "        median2 = np.median(window_data2)\n",
    "        rms2 = np.sqrt(np.mean(window_data2**2))\n",
    "#             skewdness2 = skew(window_data2)\n",
    "#             kurtos2 = kurtosis(window_data2)\n",
    "        ptp2 = np.ptp(window_data2)\n",
    "#         rang2 = np.max(window_data2) - np.min(window_data2)\n",
    "#             interq_range2 = iqr(window_data2)\n",
    "#             crest_factor2 = np.max(window_data2)/rms2\n",
    "\n",
    "        mean3 = np.mean(window_data3)\n",
    "        variance3 = np.var(window_data3)\n",
    "        median3 = np.median(window_data3)\n",
    "        rms3 = np.sqrt(np.mean(window_data3**2))\n",
    "#             skewdness3 = skew(window_data3)\n",
    "#             kurtos3 = kurtosis(window_data3)\n",
    "        ptp3 = np.ptp(window_data3)\n",
    "#         rang3 = np.max(window_data3) - np.min(window_data3)\n",
    "#             interq_range3 = iqr(window_data3)\n",
    "#             crest_factor3 = np.max(window_data3)/rms3\n",
    "\n",
    "#             time_dom = np.array([mean1,mean2,mean3,variance1,variance2,variance3,median1,median2,median3,\n",
    "#                                  rms1,rms2,rms3,skewdness1,skewdness2,skewdness3,kurtos1,kurtos2,kurtos3,\n",
    "#                                  ptp1,ptp2,ptp3,rang1,rang2,rang3,interq_range1,interq_range2,interq_range3, \n",
    "#                                  crest_factor1,crest_factor2,crest_factor3])\n",
    "        time_dom = np.array([mean1,mean2,mean3,variance1,variance2,variance3,median1,median2,median3,\n",
    "                             rms1,rms2,rms3,ptp1,ptp2,ptp3])\n",
    "        if( k == 0 ):\n",
    "            timeAcc = np.append(timeAcc, [time_dom], axis=0)\n",
    "        elif ( k == 1):\n",
    "            timeGyro = np.append(timeGyro, [time_dom], axis=0)\n",
    "\n",
    "        if slider1.reached_end_of_list(): break\n",
    "\n",
    "print(timeAcc.shape)\n",
    "print(timeGyro.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Frequency Domain\n",
    "Afx, Atx, Ax = signal.spectrogram(ax, fs=fs,window=signal.get_window('hann',25),noverlap=20,nperseg=25)\n",
    "Afy, Aty, Ay = signal.spectrogram(ay, fs=fs,window=signal.get_window('hann',25),noverlap=20,nperseg=25)\n",
    "Afz, Atz, Az = signal.spectrogram(az, fs=fs,window=signal.get_window('hann',25),noverlap=20,nperseg=25)\n",
    "\n",
    "Wfx, Wtx, Wx = signal.spectrogram(wx, fs=fs,window=signal.get_window('hann',25),noverlap=20,nperseg=25)\n",
    "Wfy, Wty, Wy = signal.spectrogram(wy, fs=fs,window=signal.get_window('hann',25),noverlap=20,nperseg=25)\n",
    "Wfz, Wtz, Wz = signal.spectrogram(wz, fs=fs,window=signal.get_window('hann',25),noverlap=20,nperseg=25)\n",
    "\n",
    "Ax = Ax.T\n",
    "Ay = Ay.T\n",
    "Az = Az.T\n",
    "Wx = Wx.T\n",
    "Wy = Wy.T\n",
    "Wz = Wz.T\n",
    "resA = np.concatenate((Ax, Ay, Az), axis = 1)\n",
    "resW = np.concatenate((Wx, Wy, Wz), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(resA.shape[0]):\n",
    "    sA = sorted(resA[j])\n",
    "    sW = sorted(resW[j])\n",
    "#     featureAcc = sA[0:15] + sA[resA.shape[1]-15:resA.shape[1]]    #For 100 features set 50 to 25\n",
    "#         featureAcc.extend( timeAcc[j] )\n",
    "#     featureGyr = sW[0:15] + sW[resA.shape[1]-15:resA.shape[1]]    #For 100 features set 50 to 25\n",
    "#         featureGyr.extend( timeGyro[j] )\n",
    "#     final = np.concatenate((featureAcc,featureGyr))\n",
    "\n",
    "    final = np.concatenate((timeAcc[j],timeGyro[j],sA,sW))\n",
    "#     print((final))\n",
    "\n",
    "#   final = np.append( final, i )\n",
    "    final = final.reshape(1,-1)\n",
    "#     print((final))\n",
    "    print(svm_model_linear.predict(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
